---
layout: single
title: "LangChain - 02. PromptTemplates" 
categories: LangChain
tag: [Python, LLM, LangChain]
toc: true
toc_sticky: true
toc_label: "ëª©ì°¨"
---

## LangChain ì´ë€?

LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

[ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/v0.2/docs/introduction/)

[ìœ„ì˜ ê³µì‹ ë¬¸ì„œë¥¼ ë³´ê³  ì—°ìŠµí•œ ê³³](https://github.com/just-record/langchain_practice)

## PromptTemplates ì´í•´ í•˜ê¸°

### 1. 'Tell me a joke about cats'ë¥¼ llmì— ìš”ì²­ í•˜ê¸°

```python
# ì•ìœ¼ë¡œ ì•„ë˜ 2ì¤„ì€ ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.
from dotenv import load_dotenv
load_dotenv()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

result = llm.invoke("Tell me a joke about cats")
print(result.content)
```

```python
# ê²°ê³¼
Why was the cat sitting on the computer?

Because it wanted to keep an eye on the mouse!
```

### 2. catsë¥¼ ë³€ìˆ˜í™” í•˜ê¸°

'Tell me a joke about {topic}'

```python
# ... ìœ„ì™€ ë™ì¼ ...

query = "Tell me a joke about {topic}".format(topic="cats")
result = llm.invoke(query)
print(result.content)
```

### 3. PromptTemplates ì‚¬ìš©í•˜ê¸°

```python
from langchain_core.prompts import PromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini")

query = "Tell me a joke about {topic}"
# topicì„ ë³€ìˆ˜í™”í•œ queryë¥¼ PromptTemplateìœ¼ë¡œ ë³€í™˜
prompt_template = PromptTemplate.from_template(query) 

### 1. prompt í™•ì¸í•˜ê¸°
print(prompt_template.format(topic="cats")) # topicì„ catsë¡œ í• ë‹¹
print(prompt_template.invoke({"topic": "cats"}))

### 2. PromptTemplateì„ chainì— ì‚¬ìš©
print('-'*50)
chain = prompt_template | llm
print(chain.invoke({"topic": "cats"}).content)
```

```python
# ê²°ê³¼
Tell me a joke about cats
text='Tell me a joke about cats'
--------------------------------------------------
Why was the cat sitting on the computer?

Because it wanted to keep an eye on the mouse!
```

## PromptTemplates ì‚¬ìš© ì˜ˆì‹œ

### 1. from_template ì‚¬ìš©

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

print(prompt_template.invoke({"topic": "cats"}))
# text='Tell me a joke about cats'
```

### 2. input_variables ì‚¬ìš©

ë³€ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate(
    template="Tell me a joke about {topic}",
    input_variables=["topic"]
)

print(prompt_template.invoke({"topic": "cats"}))
# text='Tell me a joke about cats'
```

### 3. 2ê°œ ì´ìƒì˜ ë³€ìˆ˜ ì‚¬ìš©

```python
from langchain_core.prompts import PromptTemplate

template = "Tell me a {adjective} joke about {topic}"

### ë°©ë²• 1. 
prompt_template = PromptTemplate.from_template(template)

print(prompt_template.invoke({"adjective": "funny", "topic": "joke"}))


### ë°©ë²• 2.
print('-'*50)
prompt_template = PromptTemplate(
    template="Tell me a {adjective} joke about {topic}",
    input_variables=["adjective", "topic"],
    validate_template=True # templateì— ëª¨ë“  ë³€ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€,  ì‚¬ìš©ë˜ëŠ”ì§€ í™•ì¸
)

print(prompt_template.invoke({"adjective": "funny", "topic": "joke"}))
```

```python
# ê²°ê³¼
text='Tell me a funny joke about joke'
--------------------------------------------------
text='Tell me a funny joke about joke'
```

### 4. partial_variables - ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ í• ë‹¹

templateì˜ ë³€ìˆ˜ ì¼ë¶€ë¶„ì„ ë¯¸ë¦¬ í• ë‹¹

```python
from langchain_core.prompts import PromptTemplate

template = "Tell me a {adjective} joke about {topic}"

### ë°©ë²• 1 - from_template
prompt = PromptTemplate.from_template(template)
partial_prompt = prompt.partial(adjective="funny")
print(partial_prompt.invoke({"topic": "cats"}))

### ë°©ë²• 2 - partial_variables
print('-'*30)
prompt = PromptTemplate(
    template=template, 
    input_variables=["topic"], 
    partial_variables={"adjective": "funny"}
)
print(partial_prompt.invoke({"topic": "cats"}))
```

```python
# ê²°ê³¼
text='Tell me a funny joke about cats'
------------------------------
text='Tell me a funny joke about cats'
```

### 5. partial_variables - í•¨ìˆ˜ì™€ í•¨ê»˜ ì‚¬ìš© í•˜ê¸°

```python
from langchain_core.prompts import PromptTemplate
from datetime import datetime


def _get_datetime():
    now = datetime.now()
    return now.strftime("%m/%d/%Y, %H:%M:%S")


### ë°©ë²• 1 - from_template
prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective", "date"],
)
partial_prompt = prompt.partial(date=_get_datetime)
print(partial_prompt.format(adjective="funny"))
# Tell me a funny joke about the day 07/03/2024, 19:30:23

### ë°©ë²• 2 - partial_variables
print("-" * 30)
prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective"],
    partial_variables={"date": _get_datetime},
)
print(prompt.format(adjective="funny"))
# Tell me a funny joke about the day 07/03/2024, 19:30:23
```

```python
# ê²°ê³¼
Tell me a funny joke about the day 07/03/2024, 19:30:23
------------------------------
Tell me a funny joke about the day 07/03/2024, 19:30:23
```

## ChatPromptTemplate ë€?

ëŒ€í™”í˜• AI ëª¨ë¸ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ë‹¤ì¤‘ ë©”ì‹œì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°

### PromptTemplateê³¼ ChatPromptTemplateì˜ ì°¨ì´ì 

|êµ¬ë¶„|PromptTemplate|ChatPromptTemplate|
|---|---|---|
|ì‚¬ìš© ëª©ì |ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë˜ë©´ ë‹¨ì¼ ë¬¸ìì—´ í˜•íƒœì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±|ëŒ€í™”í˜• AI ëª¨ë¸ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©´ ì—¬ëŸ¬ ë©”ì‹œì§€ë¡œ êµ¬ì„±ëœ ëŒ€í™” í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±|
|êµ¬ì¡°|ë‹¨ì¼ í…œí”Œë¦¿ ë¬¸ìì—´ë¡œ êµ¬ì„±ë˜ë©° ë³€ìˆ˜ë¥¼ ì¤‘ê´„í˜¸ {} ì•ˆì— ë„£ì–´ í‘œí˜„|ì—¬ëŸ¬ ê°œì˜ ë©”ì‹œì§€ í…œí”Œë¦¿ìœ¼ë¡œ êµ¬ì„±ë˜ë©° ê° ë©”ì‹œì§€ëŠ” ì—­í• (ì‹œìŠ¤í…œ, ì‚¬ìš©ì, AI)ê³¼ ë‚´ìš©ì„ ê°€ì§|
|ë©”ì‹œì§€ í˜•ì‹|ë‹¨ì¼ ë¬¸ìì—´ì„ ì¶œë ¥|ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ë©° ê° ë©”ì‹œì§€ëŠ” ì—­í• ê³¼ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ|
|í˜¸í™˜ì„±|ëŒ€ë¶€ë¶„ì˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ê³¼ í˜¸í™˜|ì£¼ë¡œ ëŒ€í™”í˜• ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©|

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

### 1. prompt_template invokeí•˜ì—¬ ê²°ê³¼ í™•ì¸
print(prompt_template.invoke({"topic": "cats"}))


### 2. llmê¹Œì§€ ì—°ê²°í•˜ì—¬ ê²°ê³¼ í™•ì¸
print('-'*30)
from dotenv import load_dotenv
load_dotenv()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

chain = prompt_template | llm
print(chain.invoke({"topic": "cats"}))
```

```python
# ê²°ê³¼
messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')]
------------------------------
Why was the cat sitting on the computer?

Because it wanted to keep an eye on the mouse!
```

## ChatPromptTemplate ì‚¬ìš© ì˜ˆì‹œ

### 1. from_messages

```python
from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI bot. Your name is {name}."),
    ("human", "Hello, how are you doing?"),
    ("ai", "I'm doing well, thanks!"),    # aiì˜ ë‹µë³€ì„ ì¶”ê°€
    ("human", "{user_input}"),
])

prompt_value = template.invoke(
    {
        "name": "Bob",
        "user_input": "What is your name?"
    }
)
print(prompt_value)
```

```python
# ê²°ê³¼ - {name}ê³¼ {user_input}ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´ í•¨
messages=[
    SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), 
    HumanMessage(content='Hello, how are you doing?'), 
    AIMessage(content="I'm doing well, thanks!"), 
    HumanMessage(content='What is your name?')
    ]
```

### 2. input_variables

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

template = ChatPromptTemplate(
    input_variables=["name", "user_input"],
    messages=[
        SystemMessage(content="You are a helpful AI bot. Your name is {name}."),
        HumanMessage(content="Hello, how are you doing?"),
        AIMessage(content="I'm doing well, thanks!"),
        HumanMessage(content="{user_input}"),
    ]
)

prompt_value = template.invoke(
    {
        "name": "Bob",
        "user_input": "What is your name?"
    }
)
print(prompt_value)
```

```python
# ê²°ê³¼ - {name}ê³¼ {user_input}ì„ ì‹¤ì œë¡œ ëŒ€ì²´ í•˜ì§„ ì•ŠìŒ
messages=[
    SystemMessage(content='You are a helpful AI bot. Your name is {name}.'), 
    HumanMessage(content='Hello, how are you doing?'), 
    AIMessage(content="I'm doing well, thanks!"), 
    HumanMessage(content='{user_input}')
    ]
```

## MessagesPlaceholder

íŠ¹ì •í•œ ìœ„ì¹˜ì— ë©”ì‹œì§€ ëª©ë¡ì„ ì‚½ì…í•˜ê¸°

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage


### 01. MessagesPlaceholder
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

results = prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
print(results)


### 02. "placeholder"
print('-'*30)
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}")
])

results = prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
print(results)


### 3. 2ê°œ ì´ìƒì˜ placeholder
print('-'*30)
prompt_template2 = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("human_msgs"),
    MessagesPlaceholder("ai_msgs")
])

results = prompt_template2.invoke({"human_msgs": [HumanMessage(content="hi!")], "ai_msgs": [AIMessage(content="Hello! How can I assist you today?!")]})
print(results)

```

```python
# ê²°ê³¼
messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!')]
messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!')]
messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!'), AIMessage(content='Hello! How can I assist you today?!')]false
```

## Few Shotê³¼ Example Selector

### FewShotPromptTemplate

'FewShotPromptTemplate'ì€ Few-shot í•™ìŠµì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•œë‹¤.

```python
from langchain_core.prompts import PromptTemplate

example_prompt = PromptTemplate.from_template("Question: {question}\n{answer}")

### example_prompt ì¶œë ¥
print(example_prompt)
# input_variables=['answer', 'question'] template='Question: {question}\n{answer}'

### few-shot example set
examples = [
    {
        "question": "Who lived longer, Muhammad Ali or Alan Turing?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
""",
    },
    {
        "question": "When was the founder of craigslist born?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
""",
    },
    {
        "question": "Who was the maternal grandfather of George Washington?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
""",
    },
    {
        "question": "Are both the directors of Jaws and Casino Royale from the same country?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
""",
    },
]

### example setì¤‘ í•˜ë‚˜ë¥¼ example_promptì— ì ìš©
print('-'*30)
results = example_prompt.invoke(examples[0])
print(results)
# text='Question: Who lived longer, Muhammad Ali or Alan Turing?\n\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n'


### FewShotPromptTemplate ì‚¬ìš©
print('-'*30)

from langchain_core.prompts import FewShotPromptTemplate

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(
    prompt.invoke({"input": "Who was the father of Mary Ball Washington?"}).to_string()
)
# Question: Who lived longer, Muhammad Ali or Alan Turing?

# Are follow up questions needed here: Yes.
# Follow up: How old was Muhammad Ali when he died?
# Intermediate answer: Muhammad Ali was 74 years old when he died.
# Follow up: How old was Alan Turing when he died?
# Intermediate answer: Alan Turing was 41 years old when he died.
# So the final answer is: Muhammad Ali


# Question: When was the founder of craigslist born?

# Are follow up questions needed here: Yes.
# Follow up: Who was the founder of craigslist?
# Intermediate answer: Craigslist was founded by Craig Newmark.
# Follow up: When was Craig Newmark born?
# Intermediate answer: Craig Newmark was born on December 6, 1952.
# So the final answer is: December 6, 1952


# Question: Who was the maternal grandfather of George Washington?

# Are follow up questions needed here: Yes.
# Follow up: Who was the mother of George Washington?
# Intermediate answer: The mother of George Washington was Mary Ball Washington.
# Follow up: Who was the father of Mary Ball Washington?
# Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
# So the final answer is: Joseph Ball


# Question: Are both the directors of Jaws and Casino Royale from the same country?

# Are follow up questions needed here: Yes.
# Follow up: Who is the director of Jaws?
# Intermediate Answer: The director of Jaws is Steven Spielberg.
# Follow up: Where is Steven Spielberg from?
# Intermediate Answer: The United States.
# Follow up: Who is the director of Casino Royale?
# Intermediate Answer: The director of Casino Royale is Martin Campbell.
# Follow up: Where is Martin Campbell from?
# Intermediate Answer: New Zealand.
# So the final answer is: No


# Question: Who was the father of Mary Ball Washington?
```

### SemanticSimilarityExampleSelector

'SemanticSimilarityExampleSelector'ëŠ” ì£¼ì–´ì§„ ì…ë ¥ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì œ(Few shotì„ ìœ„í•œ example set ì¤‘ì—ì„œ)ë¥¼ ì„ íƒí•œë‹¤. ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì œë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ ì„ë² ë”© ë²¡í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ì„ë² ë”© ë²¡í„°ëŠ” openaiì˜ 'OpenAIEmbeddings'ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ openaiì˜ API í‚¤ê°€ í•„ìš”í•˜ë‹¤. ë˜í•œ, ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ 'Chroma'ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
# ... ìœ„ì˜ ì½”ë“œ examples ì •ì˜ ê¹Œì§€ëŠ” ì½”ë“œê°€ ë™ì¼í•¨ ...
from dotenv import load_dotenv
load_dotenv()

from langchain_core.example_selectors import SemanticSimilarityExampleSelector

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # This is the list of examples available to select from.
    examples,
    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # This is the number of examples to produce.
    k=1,
)

# Select the most similar example to the input.
question = "Who was the father of Mary Ball Washington?"
selected_examples = example_selector.select_examples({"question": question})
print(f"Examples most similar to the input: {question}")


### 1. ê°€ì¥ ìœ ì‚¬í•œ example ì¶œë ¥
for example in selected_examples:
    print("\n")
    for k, v in example.items():
        print(f"{k}: {v}")


### 2. FewShotPromptTemplateì— ì ìš©
print('-'*30)

from langchain_core.prompts import FewShotPromptTemplate

prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(
    prompt.invoke({"input": "Who was the father of Mary Ball Washington?"}).to_string()
)
```

```python
# ê²°ê³¼
Examples most similar to the input: Who was the father of Mary Ball Washington?


answer: 
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball

question: Who was the maternal grandfather of George Washington?
------------------------------
Question: Who was the maternal grandfather of George Washington?

Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball


Question: Who was the father of Mary Ball Washington?
```

### FewShotChatMessagePromptTemplate

'FewShotChatMessagePromptTemplate'ì€ ëŒ€í™”í˜• AI ëª¨ë¸ì„ ìœ„í•œ Few-shot í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•œë‹¤.

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0.0)

### 1. ì •ì˜ ë˜ì§€ ì•Šì€ 'ğŸ¦œ'ë¡œ ì—°ì‚°í•˜ê¸°
print(model.invoke("What is 2 ğŸ¦œ 9?").content)
# he expression "2 ğŸ¦œ 9" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji ğŸ¦œ followed by the number 9. It does not have a specific mathematical meaning.

### 2. 'ğŸ¦œ'ì—°ì‚°ì„ Chat Few Shot Promt ë§Œë“¤ê¸°
print('-'*30)
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

examples = [
    {"input": "2 ğŸ¦œ 2", "output": "4"},
    {"input": "2 ğŸ¦œ 3", "output": "5"},
]

# This is a prompt template used to format each individual example.
example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
    input_variables=[],
)

print(few_shot_prompt.invoke({}).to_messages())
# [HumanMessage(content='2 ğŸ¦œ 2'), AIMessage(content='4'), HumanMessage(content='2 ğŸ¦œ 3'), AIMessage(content='5')]

### 3. Chat Few Shot Promptë¥¼ ì‚¬ìš©í•˜ì—¬ ChatOpenAIì— ì ìš©í•˜ê¸°
print('-'*30)
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

from langchain_openai import ChatOpenAI

chain = final_prompt | model

print(chain.invoke({"input": "What is 2 ğŸ¦œ 9?"}).content)
# 11
```

```python
# ê²°ê³¼
he expression "2 ğŸ¦œ 9" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji ğŸ¦œ followed by the number 9. It does not have a specific mathematical meaning.
------------------------------
[HumanMessage(content='2 ğŸ¦œ 2'), AIMessage(content='4'), HumanMessage(content='2 ğŸ¦œ 3'), AIMessage(content='5')]
------------------------------
11
```

### SemanticSimilarityExampleSelectorê³¼ FewShotChatMessagePromptTemplate

```python
from dotenv import load_dotenv
load_dotenv()

from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

examples = [
    {"input": "2 ğŸ¦œ 2", "output": "4"},
    {"input": "2 ğŸ¦œ 3", "output": "5"},
    {"input": "2 ğŸ¦œ 4", "output": "6"},
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?",
    },
]

to_vectorize = [" ".join(example.values()) for example in examples]
print(to_vectorize)
# ['2 ğŸ¦œ 2 4', '2 ğŸ¦œ 3 5', '2 ğŸ¦œ 4 6', 'What did the cow say to the moon? nothing at all', 'Write me a poem about the moon One for the moon, and one for me, who are we to talk about the moon?']
embeddings = OpenAIEmbeddings()
# ë²¡í„° DB(Chroma) ì— ì €ì¥
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)


### 1. example selector ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì œ ì„ íƒí•˜ê¸°
print('-'*30)

example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2,
)
# The prompt template will load examples by passing the input do the `select_examples` method
print(example_selector.select_examples({"input": "horse"}))
# [{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'}, {'input': '2 ğŸ¦œ 4', 'output': '6'}]


### 2. example selectorê°€ í¬í•¨ëœ Chat Prompt Template ë§Œë“¤ê¸°
print('-'*30)

from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

few_shot_prompt = FewShotChatMessagePromptTemplate(
    # The input variables select the values to pass to the example_selector
    input_variables=["input"],
    example_selector=example_selector,
    # Define how each example will be formatted.
    # In this case, each example will become 2 messages:
    # 1 human, and 1 AI
    example_prompt=ChatPromptTemplate.from_messages(
        [("human", "{input}"), ("ai", "{output}")]
    ),
)

print(few_shot_prompt.invoke(input="What's 3 ğŸ¦œ 3?").to_messages())
# What's 3 ğŸ¦œ 3?ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì œ 2ê°œë¥¼ ë°˜í™˜
# [HumanMessage(content='2 ğŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ğŸ¦œ 4'), AIMessage(content='6')]

print('-'*30)
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

print(few_shot_prompt.invoke(input="What's 3 ğŸ¦œ 3?"))
# messages=[HumanMessage(content='2 ğŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ğŸ¦œ 4'), AIMessage(content='6')]

### Use with an chat model
print('-'*30)
from langchain_openai import ChatOpenAI

chain = final_prompt | ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)
print(chain.invoke({"input": "What's 3 ğŸ¦œ 3?"}).content)
# 6
```

## PipelinePromptTemplate

'PipelinePromptTemplate'ì€ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•œë‹¤.

```python
from langchain_core.prompts import PromptTemplate

### 1. ë‹¨ìˆœ ë¬¸ìì—´ ì—°ê²°
prompt = (
    PromptTemplate.from_template("Tell me a joke about {topic}")
    + ", make it funny"
    + "\n\nand in {language}"
)

print(prompt)
# input_variables=['language', 'topic'] template='Tell me a joke about {topic}, make it funny\n\nand in {language}'

print('-'*30)
print(prompt.format(topic="sports", language="spanish"))
# Tell me a joke about sports, make it funny

# and in spanish


### 2. Chat prompt ì—°ê²° - ë¦¬ìŠ¤íŠ¸ì˜ '+' ì—°ì‚°
print('-'*30)
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

prompt = SystemMessage(content="You are a nice pirate")
new_prompt = (
    prompt + HumanMessage(content="hi") + AIMessage(content="what?") + "{input}"
)
print(new_prompt.format_messages(input="i said hi"))
# [SystemMessage(content='You are a nice pirate'), HumanMessage(content='hi'), AIMessage(content='what?'), HumanMessage(content='i said hi')]


### 3. PipelinePrompt ì‚¬ìš©
print('-'*30)
from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate

full_template = """{introduction}

{example}

{start}"""
full_prompt = PromptTemplate.from_template(full_template)

introduction_template = """You are impersonating {person}."""
introduction_prompt = PromptTemplate.from_template(introduction_template)

example_template = """Here's an example of an interaction:

Q: {example_q}
A: {example_a}"""
example_prompt = PromptTemplate.from_template(example_template)

start_template = """Now, do this for real!

Q: {input}
A:"""
start_prompt = PromptTemplate.from_template(start_template)

input_prompts = [
    ("introduction", introduction_prompt),
    ("example", example_prompt),
    ("start", start_prompt),
]
pipeline_prompt = PipelinePromptTemplate(
    final_prompt=full_prompt, pipeline_prompts=input_prompts
)

print(pipeline_prompt.input_variables)
# ['example_q', 'person', 'example_a', 'input']

print('-'*30)
print(
    pipeline_prompt.format(
        person="Elon Musk",
        example_q="What's your favorite car?",
        example_a="Tesla",
        input="What's your favorite social media site?",
    )
)
# You are impersonating Elon Musk.

# Here's an example of an interaction:

# Q: What's your favorite car?
# A: Tesla

# Now, do this for real!

# Q: What's your favorite social media site?
# A:
```

- ì—¬ëŸ¬ ê°œì˜ í•˜ìœ„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜ - (introduction, example, start)
- í•˜ìœ„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ í•˜ë‚˜ì˜ ìµœì¢… í…œí”Œë¦¿(full_template)ì— ì¡°í•©
- 'PipelinePromptTemplate'ì„ ì‚¬ìš©í•˜ì—¬ ìœ„ì˜ ê³¼ì •ì„ ìë™í™”

## íŒŒì¼ê³¼ LangSmith hubì—ì„œ PromptTemplates ì½ê¸°

`prompt_basic.yaml` íŒŒì¼

```yaml
_type: "prompt"
template: "Tell me a joke about {topic}"
input_variables: ["topic"]
```

```python
### 01. íŒŒì¼ì—ì„œ PromptTemplates ì½ê¸°
from langchain.prompts import load_prompt


file_path_prompt = './prompt_basic.yaml'

prompt_template = load_prompt(file_path_prompt)
print(prompt_template.invoke({"topic": "cats"}).to_messages())

### 2. LangSmith hubì—ì„œ PromptTemplates ì½ê¸°
print('='*30)
from langchain import hub

prompt_template = hub.pull("jbsh/langchain_practice_basic", api_url="https://api.hub.langchain.com")
print(prompt_template)
print('-'*30)
print(prompt_template.invoke({"topic": "cats"}).to_messages())
```

```python
# ê²°ê³¼
[HumanMessage(content='Tell me a joke about cats')]
==============================
input_variables=['topic'] metadata={'lc_hub_owner': 'jbsh', 'lc_hub_repo': 'langchain_practice_basic', 'lc_hub_commit_hash': '240f00963407c13026b1e00d32990a5e3117e63d47a911f02ec3fe3a77d7e543'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a chatbot.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a joke about {topic}'))]
------------------------------
[SystemMessage(content='You are a chatbot.'), HumanMessage(content='Tell me a joke about cats')]
```

---

í•´ì‹œíƒœê·¸: #LangChain #PromptTemplates #ChatPromptTemplate #from_messages #input_variables #partial_variables #MessagesPlaceholder #SystemMessage #HumanMessage #AIMessage #FewShotPromptTemplate #SemanticSimilarityExampleSelector #FewShotChatMessagePromptTemplate #PipelinePromptTemplate #LangSmith_hub
