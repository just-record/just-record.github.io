---
layout: single
title: "Ollama - 01. ì„¤ì¹˜ ë° ê¸°ë³¸ ì‚¬ìš©" 
categories: Ollama
tag: [Ollama]
toc: true
toc_sticky: true
toc_label: "ëª©ì°¨"
---

## Ollamaë€?

<https://ollama.com/>

ollamaëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(Large Language Models, LLMs)ì„ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ë³µì¡í•œ AI ëª¨ë¸ì„ ê°œì¸ìš© ì»´í“¨í„°ë‚˜ ì„œë²„ì—ì„œ ê°„í¸í•˜ê²Œ êµ¬ë™í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

âœ”ï¸ ì£¼ìš” ëª©ì 

- LLMsì˜ ì ‘ê·¼ì„± í–¥ìƒ
- ë¡œì»¬ í™˜ê²½ì—ì„œì˜ AI ëª¨ë¸ ì‹¤í–‰ ê°„ì†Œí™”
- ë‹¤ì–‘í•œ AI ëª¨ë¸ì˜ ì‰¬ìš´ ê´€ë¦¬ ë° ì‚¬ìš©

âœ”ï¸ ollamaì˜ íŠ¹ì§•ê³¼ ì¥ì 

- ë¡œì»¬ ì‹¤í–‰
  - ì¸í„°ë„· ì—°ê²° ì—†ì´ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
  - ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë° ë³´ì•ˆ ê°•í™”
  - ì§€ì—° ì‹œê°„ ê°ì†Œë¡œ ë¹ ë¥¸ ì‘ë‹µ ì†ë„

- ì‚¬ìš© í¸ì˜ì„±
  - ê°„ë‹¨í•œ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
  - ë³µì¡í•œ ì„¤ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
  - ì‚¬ìš©ì ì¹œí™”ì ì¸ CLI ì¸í„°í˜ì´ìŠ¤

- ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›:
  - Llama 3.1, Phi 3, Mistral, Gemma 2 ë“± ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì§€ì›
  - ì»¤ìŠ¤í…€ ëª¨ë¸ ì¶”ê°€ ê¸°ëŠ¥

- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±:
  - ìµœì í™”ëœ ì‹¤í–‰ìœ¼ë¡œ í•˜ë“œì›¨ì–´ ìì› íš¨ìœ¨ì  ì‚¬ìš©
  - ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì— ë§ì¶¤ ì‹¤í–‰ ê°€ëŠ¥

- API ì§€ì›:
  - RESTful APIë¥¼ í†µí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì—°ë™
  - ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°€ëŠ¥

- ì»¤ë®¤ë‹ˆí‹° ë° ì˜¤í”ˆì†ŒìŠ¤:
  - í™œë°œí•œ ê°œë°œì ì»¤ë®¤ë‹ˆí‹°
  - ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ ë° ê°œì„ 

- ê²½ëŸ‰í™” ë° ì–‘ìí™”:
  - ëª¨ë¸ í¬ê¸° ì¶•ì†Œ ë° ì„±ëŠ¥ ìµœì í™” ê¸°ëŠ¥
  - ì œí•œëœ ë¦¬ì†ŒìŠ¤ì—ì„œë„ ê³ ì„±ëŠ¥ ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥

- ë©€í‹° ëª¨ë¸ ê´€ë¦¬:
  - ì—¬ëŸ¬ ëª¨ë¸ì„ ì‰½ê²Œ ì „í™˜í•˜ë©° ì‚¬ìš© ê°€ëŠ¥
  - ë‹¤ì–‘í•œ ì‘ì—…ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ ìš©ì´

## ì„¤ì¹˜

<https://ollama.com/download>

âœ”ï¸ Linux (Ubuntu 22.04 LTS)

```bash
# Update package list and upgrade existing packages
sudo apt update && sudo apt upgrade -y

# Install required dependencies
sudo apt install curl -y

# Download Ollama install script
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version
# ollama version is 0.3.5
```

âœ”ï¸ ollama ì—…ê·¸ë ˆì´ë“œ

ì´ ë¬¸ì„œë¥¼ ì‘ì„± í•˜ëŠ” ì¤‘ì— ollamaê°€ ì—…ê·¸ë ˆì´ë“œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì—…ê·¸ë ˆì´ë“œ í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ ë³´ì•˜ìŠµë‹ˆë‹¤.

<https://github.com/ollama/ollama/blob/main/docs/faq.md>

ì„¤ì¹˜ì™€ ë™ì¼í•œ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš© í•˜ë©´ ë©ë‹ˆë‹¤.

```bash
curl -fsSL https://ollama.com/install.sh | sh
# ì„¤ì¹˜ í›„ ë²„ì „ í™•ì¸
ollama --version
# ollama version is 0.3.6 => 0.3.5 ì—ì„œ 0.3.6ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ ë˜ì—ˆìŠµë‹ˆë‹¤.
```

## ì„œë¹„ìŠ¤ ì‹œì‘ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, ì‹¤í–‰

âœ”ï¸ ì„œë¹„ìŠ¤ ì‹œì‘

```bash
ollama serve
```

ìœ„ì˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ë§Œ í–ˆëŠ”ë° ì´ë¯¸ ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì— ìˆì—ˆë‹¤. ìŒ~ ì´ì „ì— ë‚´ê°€ ì„¤ì¹˜ë¥¼ í–ˆì—ˆë‚˜?

```bash
ollama serve
# Error: listen tcp 127.0.0.1:11434: bind: address already in use

sudo ss -lptn 'sport = :11434'
# ... 127.0.0.1:11434 ...  users:(("ollama",pid= ...

# reboot ì„ í•´ë„ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ê²ƒ ê°™ë‹¤.
```

âœ”ï¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
ollama pull llama3.1
```

ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

- <https://ollama.com/library>ì—ì„œ ì›í•˜ëŠ” ëª¨ë¸ì„ ê²€ìƒ‰ í›„ ì„ íƒ
- ì¢Œì¸¡ 'select box'ì—ì„œ ì›í•˜ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆ ì„ íƒ -> ìš°ì¸¡ì— ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸

ì˜ˆì‹œ: llama3.1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

- <https://ollama.com/library/llama3.1>
  - '8b' -> 'ollama run llama3.1'
  - '70b' -> 'ollama run llama3.1:70b'
  - '405b' -> 'ollama run llama3.1:405b'

ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•˜ì§€ ì•Šê³  ë°”ë¡œ ì‹¤í–‰ í•˜ë©´ ë‹¤ìš´ë¡œë“œ í›„ì— ì‹¤í–‰ ëœë‹¤.

âœ”ï¸ ëª¨ë¸ ì‹¤í–‰

```bash
ollama run llama3.1
# ollama run gemma2
```

ëª¨ë¸ì„ ì‹¤í–‰ í•˜ë©´ `>>>` í”„ë¡¬í”„íŠ¸ê°€ ë‚˜ì˜¤ê³  ì…ë ¥ì„ ë°›ì„ ìˆ˜ ìˆë‹¤. ì°¸ê³ ë¡œ **ëŒ€í™”í˜• ëª¨ë“œ**ì…ë‹ˆë‹¤.

```plaintext
>>> hi
How's it going? Is there something I can help you with or would you like to chat?

>>> ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼.
Nice to meet you, í™ê¸¸ë™ë‹˜!

í™ê¸¸ë™ì€ í•œêµ­ì˜ ì „ì„¤ì ì¸ ì²­ì¶˜ì¸ í™ê¸¸ë™ì´ë¼ëŠ” ìì´ë¸Œì—ì„œ ìœ ë˜í•œ ì´ë¦„ì…ë‹ˆë‹¤. ê·¸ëŸ¼, í™ê¸¸ë™ë‹˜ì´ ê¶ê¸ˆ í•œ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

>>> ë‚´ ì´ë¦„ì´ ë­ì•¼?
í™ê¸¸ë™ë‹˜ì´ ìê¸° ì´ë¦„ì„ ë¬¼ìœ¼ì…¨ë‹¤êµ¬ìš”?

 ë‚´ ì´ë¦„ì€ ë‚˜ë¡œêµ°ìš”! (I'm just a computer program, I don't have a real name)
```

âœ”ï¸ ëª¨ë¸ ì‹¤í–‰ ì¢…ë£Œ
  
```bash
>>> /bye
# ë˜ëŠ”
# Ctrl + d
```

## ëª…ë ¹ì–´

### ê¸°ë³¸ ëª…ë ¹ì–´

```bash
# ë„ì›€ë§
ollama --help
# ë²„ì „ í™•ì¸
ollama --version
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
ollama list
# ëª¨ë¸ ì •ë³´ í™•ì¸
ollama show [ëª¨ë¸ëª…] # llama3.1
# ëª¨ë¸ ì‚­ì œ
ollama rm [ëª¨ë¸ëª…] # llama3.1
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull [ëª¨ë¸ëª…] # llama3.1
# ëª¨ë¸ ì‹¤í–‰
ollama run [ëª¨ë¸ëª…] # llama3.1
# ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰
ollama run [ëª¨ë¸ëª…] [ì¿¼ë¦¬]
### ollama run llama3.1 "Tell me about Ollama"
```

### ëª¨ë¸ ì‹¤í–‰ ì¤‘ ëª…ë ¹ì–´

```bash
# ë„ì›€ë§
/help
# ì¢…ë£Œ
/bye
# ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”
/clear
# ëª¨ë¸ ë¡œë“œ
/load [ëª¨ë¸ëª…] # llama3.1
# í˜„ì¬ ì„¸ì…˜ ì €ì¥
/save [ëª¨ë¸ëª…]
# ëª¨ë¸ ìƒì„¸ ì •ë³´ í™•ì¸
/show info
# ëª¨ë¸ ë¼ì´ì„ ìŠ¤ í™•ì¸
/show license
# ëª¨ë¸ì˜ Modelfile í™•ì¸
/show modelfile
# ëª¨ë¸ì˜ Parameters í™•ì¸
/show parameters
# ëª¨ë¸ì˜ system ë©”ì‹œì§€ í™•ì¸
/show system
# ëª¨ë¸ì˜ Prompt Template í™•ì¸
/show template
```

- `/load` ì˜ˆì‹œ
  - llama3.1 ëª¨ë¸ì„ ì‚¬ìš© í•˜ë‹¤ê°€
  - `/load gemma2`ë¥¼ ì‹¤í–‰ í•˜ë©´ gemma2 ëª¨ë¸ë¡œ ë³€ê²½ëœë‹¤.
  - `/show info`ë¥¼ ì‹¤í–‰ í•˜ë©´ gemma2 ëª¨ë¸ ì •ë³´ê°€ ë‚˜ì˜¨ë‹¤.
- `/save` ì˜ˆì‹œ
  - gemma2 ëª¨ë¸ì„ ì‚¬ìš© í•˜ì—¬ ëŒ€í™”ë¥¼ í•˜ë‹¤ê°€
  - `/save gemma2_chathistory`ë¥¼ ì‹¤í–‰ í•˜ë©´ ì§€ê¸ˆ ê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ëª¨ë¸ì´ ì €ì¥ëœë‹¤.
  - `ollama run gemma2_chathistory`ë¥¼ ì‹¤í–‰ í•˜ë©´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì‹¤í–‰ ëœë‹¤.
  - ì•„ë˜ëŠ” ë‹¤ì‹œ ì‹¤í–‰ í–ˆì„ ë•Œì˜ ì˜ˆì‹œì´ë‹¤.

```plaintext
ollama run gemma2_chathistory
>>> Hi
Hello! ğŸ‘‹ How can I help you today? ğŸ˜Š

>>> My name is KimHun
Hi KimHun, it's nice to meet you!

What can I do for you? ğŸ˜„  Do you have any questions I can answer or tasks I can help with?

>>> What is my name?
Your name is KimHun. ğŸ˜Š  I remembered that from when you introduced yourself!  Is there anything else I can help you with?  </p>
```

## API ì‚¬ìš©

ollamaëŠ” HTTPë¥¼ í†µí•´ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” RESTful APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ API ì„œë²„ëŠ” <http://localhost:11434>ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

âœ”ï¸ ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸

- ìƒì„± (Generate): POST /api/generate - í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸
- ì±„íŒ… (Chat): POST /api/chat - ëŒ€í™”í˜• ì‘ë‹µì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸
- ì„ë² ë”© (Embeddings): POST /api/embeddings - í…ìŠ¤íŠ¸ì˜ ë²¡í„° í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
- ëª¨ë¸ ëª©ë¡ (List Models): GET /api/tags - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜
- ëª¨ë¸ ìƒì„± (Create Model): POST /api/create - ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ëª¨ë¸ì„ ìˆ˜ì •
- ëª¨ë¸ ì‚­ì œ (Delete Model): DELETE /api/delete - ëª¨ë¸ ì‚­ì œ

### `curl` ì‚¬ìš©

> í…ìŠ¤íŠ¸ ìƒì„±

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "Why is the sky blue?"
}'
```

```json
{"model":"llama3.1","created_at":"2024-08-14T08:51:16.105202552Z","response":"The","done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:51:16.105206718Z","response":" sky","done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:51:16.10526969Z","response":" appears","done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:51:16.105271607Z","response":" blue","done":false}
...
```

> ì±„íŒ…

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1",
  "messages": [
    { "role": "user", "content": "Hello, how are you?" }
  ]
}'
```

```json
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.185224768Z","message":{"role":"assistant","content":"I"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.185229236Z","message":{"role":"assistant","content":"'m"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.185306075Z","message":{"role":"assistant","content":" just"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.186735033Z","message":{"role":"assistant","content":" a"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.194470254Z","message":{"role":"assistant","content":" language"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T08:52:47.20219522Z","message":{"role":"assistant","content":" model"},"done":false}
...
```

> ì„ë² ë”©

```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "llama3.1",
  "prompt": "Hello world"
}'
```

```json
{"embedding":[-1.0290963649749756,-2.656991958618164,0.6437098383903503,-0.1601492166519165,2.6510043144226074,...]}
```

> ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

```bash
curl http://localhost:11434/api/tags
```

```json
{"models":[{"name":"llama3.1:70b","model":"llama3.1:70b",...}}]}
```

### `Python` ì‚¬ìš©

```python
import requests
import json

def generate_text(prompt, model="llama3.1"):
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt
    }
    response = requests.post(url, json=data)
    return response.text

def chat(messages, model="llama3.1"):
    url = "http://localhost:11434/api/chat"
    data = {
        "model": model,
        "messages": messages
    }
    response = requests.post(url, json=data)
    return response.text

def embedding(messages, model="llama3.1"):
    url = "http://localhost:11434/api/embeddings"
    data = {
        "model": model,
        "messages": messages
    }
    response = requests.post(url, json=data)
    return response.text

def get_models():
    url = "http://localhost:11434/api/tags"
    response = requests.get(url)
    return response.text

### í…ìŠ¤íŠ¸ ìƒì„±
print(generate_text("Why is the sky blue?"))

### ì±„íŒ…
print('-'*30)
chat_messages = [
    {"role": "user", "content": "Hello, how are you?"}
]
print(chat(chat_messages))

### ì„ë² ë”©
print('-'*30)
print(embedding("Hello world"))

### ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
print('-'*30)
print(get_models())
```

```python
# ê²°ê³¼
{"model":"llama3.1","created_at":"2024-08-14T09:06:05.569219117Z","response":"The","done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:05.569221959Z","response":" sky","done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:05.569285853Z","response":" appears","done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:05.569286898Z","response":" blue","done":false}
...
------------------------------
{"model":"llama3.1","created_at":"2024-08-14T09:06:08.581256307Z","message":{"role":"assistant","content":"I"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:08.581263539Z","message":{"role":"assistant","content":"'m"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:08.581331748Z","message":{"role":"assistant","content":" doing"},"done":false}
{"model":"llama3.1","created_at":"2024-08-14T09:06:08.581333592Z","message":{"role":"assistant","content":" well"},"done":false}
...
------------------------------
{"embedding":[-1.0290963649749756,-2.656991958618164,0.6437098383903503,...]}
------------------------------
{"models":[{"name":"llama3.1:70b","model":"llama3.1:70b",...}]}
```

### LanaChain ì‚¬ìš©

<https://python.langchain.com/v0.2/docs/integrations/llms/ollama/>

```python
# ì„¤ì¹˜
pip install -U langchain
pip install -U langchain-community
```

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama


template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOllama(model="llama3.1")

chain = prompt | model

response = chain.invoke({"question": "What is LangChain?"})
print(response)
```

## ì›ê²©ìœ¼ë¡œ API ìš”ì²­

ollamaê°€ ì„¤ì¹˜ ë˜ì§€ ì•Šì€ ë‹¤ë¥¸ ì„œë²„ì—ì„œ ollama APIë¥¼ í˜¸ì¶œ í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´ ë•Œ ì™¸ë¶€ ì„œë²„ì—ì„œ ollamaì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì • í•´ì•¼ í•œë‹¤.

âœ”ï¸ ubuntu 22.04 LTS

ë¨¼ì € `11434` í¬íŠ¸ëŠ” ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ë°©í™”ë²½ì„ ì—´ê¸°

```bash
sudo ufw allow 11434
# í™•ì¸
sudo ufw status
```

ì™¸ë¶€ì—ì„œ ollamaì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •

```bash
### `systemd`ë¡œ ì„œë¹„ìŠ¤ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
sudo mkdir -p /etc/systemd/system/ollama.service.d
### ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ì˜ ì„¤ì •ì„ ì¬ì •ì˜í•˜ê±°ë‚˜ í™•ì¥í•˜ê¸° ìœ„í•´ environment.conf íŒŒì¼ì„ ìƒì„±
echo '[Service]' | sudo tee -a /etc/systemd/system/ollama.service.d/environment.conf
### í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
echo 'Environment="OLLAMA_HOST=0.0.0.0:11434"' | sudo tee -a /etc/systemd/system/ollama.service.d/environment.conf
### CORS ì„¤ì •ì„ í†µí•´ ëª¨ë“  ë„ë©”ì¸ì—ì„œ ì ‘ê·¼ì„ í—ˆìš©
# echo 'Environment="OLLAMA_ORIGINS=*"' | sudo tee -a /etc/systemd/system/ollama.service.d/environment.conf
```

Systemd ë°ëª¬ ì¬ë¡œë“œ ë° ì„œë¹„ìŠ¤ ì¬ì‹œì‘
  
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

ëª¨ë¸ ì‹¤í–‰

```bash
ollama run llama3.1
```

ì™¸ë¶€ ì ‘ê·¼ í™•ì¸

```python
import requests


def get_models():
    # url = "http://localhost:11434/api/tags"
    url = "http://ë„ë©”ì¸ì£¼ì†Œ:ì™¸ë¶€í¬íŠ¸/api/tags"
    response = requests.get(url)
    return response.text


print(get_models())
```

### LanaChainì—ì„œ  ì›ê²©ìœ¼ë¡œ API ìš”ì²­

ollamaê°€ ì„¤ì¹˜ ë˜ì§€ ì•Šì€ ì„œë²„ì—ì„œ LanaChainì„ ì‚¬ìš©í•˜ì—¬ ì›ê²©ì˜ ollama APIë¥¼ í˜¸ì¶œ í•˜ê¸°.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama


template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOllama(
    model="llama3.1",
    base_url="http://ë„ë©”ì¸ì£¼ì†Œ:ì™¸ë¶€í¬íŠ¸")

chain = prompt | model

response = chain.invoke({"question": "What is LangChain?"})
print(response)
```

---

í•´ì‹œíƒœê·¸: #Ollama #ì„¤ì¹˜ #ê¸°ë³¸ì‚¬ìš© #APIì‚¬ìš© #í…ìŠ¤íŠ¸ìƒì„± #ì±„íŒ… #ì„ë² ë”© #ëª¨ë¸ëª©ë¡ì¡°íšŒ #curl #Python #LanaChain #ì™¸ë¶€APIì‚¬ìš©
