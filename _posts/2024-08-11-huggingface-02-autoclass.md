---
layout: single
title: "Hugging Face - 02. Transformers - AutoClass" 
categories: Hugging-Face
tag: [Hugging-Face, Transformers, AutoClass]
toc: true
toc_sticky: true
toc_label: "ëª©ì°¨"
---

## Transformer

ê³µì‹ë¬¸ì„œ: <https://huggingface.co/docs/transformers/index>

ìœ„ì˜ ê³µì‹ ë¬¸ì„œë¥¼ ì—°ìŠµ í•œ ê³³: <https://github.com/just-record/huggingface_practice>

ğŸ¤— TransformersëŠ” ìµœì‹ ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆëŠ” APIì™€ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install transformers, datasets, evaluate, accelerate
pip install torch       # PyTorch
pip install tensorflow  # TensorFlow
```

## AutoClass

<https://huggingface.co/docs/transformers/autoclass_tutorial>

AutoClassëŠ” ì£¼ì–´ì§„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì˜¬ë°”ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì¶”ë¡ í•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤. from_pretrained() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë„ ë¹ ë¥´ê²Œ ë¡œë“œí•  ìˆ˜ ìˆì–´, ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë° ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë“¤ì¼ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´, í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‘ë™í•˜ëŠ” ì½”ë“œëŠ” ì•„í‚¤í…ì²˜ê°€ ë‹¤ë¥´ë”ë¼ë„ ìœ ì‚¬í•œ ì‘ì—…ì„ ìœ„í•´ í›ˆë ¨ëœ ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œë„ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.

- Architecture (ì•„í‚¤í…ì²˜): ì•„í‚¤í…ì²˜ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ "ë¼ˆëŒ€"ë¥¼ ì˜ë¯¸
  - ì˜ˆë¥¼ ë“¤ì–´, BERT, GPT, ResNet ë“±ì´ ê°ê° ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
- Checkpoint (ì²´í¬í¬ì¸íŠ¸): íŠ¹ì • ì•„í‚¤í…ì²˜ì— ëŒ€í•´ í•™ìŠµëœ ê°€ì¤‘ì¹˜(weights)ì˜ ì§‘í•©
  - ì˜ˆë¥¼ ë“¤ì–´, 'google-bert/bert-base-uncased'ëŠ” BERT ì•„í‚¤í…ì²˜ì˜ í•œ ì²´í¬í¬ì¸íŠ¸
- Model (ëª¨ë¸): ì•„í‚¤í…ì²˜ë‚˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ì§€ì¹­í•  ìˆ˜ ìˆì–´ ë¬¸ë§¥ì— ë”°ë¼ ëª¨ë¸ì´ ì•„í‚¤í…ì²˜ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆê³ , íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆìŒ
  - ì˜ˆë¥¼ ë“¤ì–´, "BERT ëª¨ë¸"ì´ë¼ê³  í•  ë•ŒëŠ” BERT ì•„í‚¤í…ì²˜ë¥¼ ì˜ë¯¸í•  ìˆ˜ ìˆì§€ë§Œ, "ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ë¡œë“œí–ˆë‹¤"ë¼ê³  í•˜ë©´ íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¥¼ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

## AutoTokenizer

ê±°ì˜ ëª¨ë“  NLP ì‘ì—…ì€ í† í¬ë‚˜ì´ì €ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì…ë ¥ì„ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
# AutoTokenizerì— ì˜í•´ ì„ íƒëœ í† í¬ë‚˜ì´ì € ì¡°íšŒ
print(tokenizer)

print('-'*30)
sequence = "In a hole in the ground there lived a hobbit."
# sequenceë¥¼ ìë™ìœ¼ë¡œ ì„ íƒ ëœ tokenizerì— ì˜í•´ í† í°í™”
print(tokenizer(sequence))
```

```python
# ê²°ê³¼
BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
        0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
        103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
------------------------------
{
'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

## AutoImageProcessor

ë¹„ì „ ì‘ì—…ì˜ ê²½ìš°, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ë¥¼ ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

print(image_processor)

print('-'*30)
import requests
from PIL import Image
import io

image_url="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
response = requests.get(image_url)
image = Image.open(io.BytesIO(response.content))

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
inputs = image_processor(images=image, return_tensors="pt")

print("ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ shape:", inputs.pixel_values.shape)
print("ì…ë ¥ ì´ë¯¸ì§€ì˜ íƒ€ì…:", type(inputs))

# ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ì˜ ì£¼ìš” ì†ì„± ì¶œë ¥
for key, value in inputs.items():
    print(f"{key}: {type(value)} - shape: {value.shape}")
```

```python
# ê²°ê³¼
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.
ViTImageProcessor {
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "ViTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 224,
    "width": 224
  }
}
------------------------------
ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ shape: torch.Size([1, 3, 224, 224])
ì…ë ¥ ì´ë¯¸ì§€ì˜ íƒ€ì…: <class 'transformers.image_processing_base.BatchFeature'>
pixel_values: <class 'torch.Tensor'> - shape: torch.Size([1, 3, 224, 224])
```

## AutoBackbone

AutoBackboneì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë°±ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë°±ë³¸ì˜ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ íŠ¹ì§• ë§µì„ ì–»ì„ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

```python
# pip install Pillow
from transformers import AutoImageProcessor, AutoBackbone
import torch
from PIL import Image
import requests


url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))

inputs = processor(image, return_tensors="pt")
outputs = model(**inputs)
feature_maps = outputs.feature_maps

print(feature_maps[0].shape)
```

```python
# ê²°ê³¼
torch.Size([1, 96, 56, 56])
# ì²« ë²ˆì§¸ ì°¨ì› (1): ë°°ì¹˜ í¬ê¸°
# ë‘ ë²ˆì§¸ ì°¨ì› (96): ì±„ë„ ìˆ˜ ë˜ëŠ” íŠ¹ì„± ë§µ(feature map)ì˜ ìˆ˜
# ì„¸ ë²ˆì§¸ ì°¨ì› (56): ë†’ì´
# ë„¤ ë²ˆì§¸ ì°¨ì› (56): ë„ˆë¹„
```

- 'out_indices': íŠ¹ì§• ë§µì„ ì–»ê³ ì í•˜ëŠ” ë ˆì´ì–´ì˜ ì¸ë±ìŠ¤
- 'out_features': íŠ¹ì§• ë§µì„ ì–»ê³ ì í•˜ëŠ” ë ˆì´ì–´ì˜ ì´ë¦„

## AutoFeatureExtractor

ì˜¤ë””ì˜¤ ì‘ì—…ì˜ ê²½ìš°, íŠ¹ì„± ì¶”ì¶œê¸°ê°€ ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
from transformers import AutoFeatureExtractor
import librosa
import requests

feature_extractor = AutoFeatureExtractor.from_pretrained("ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition")

# ì˜¤ë””ì˜¤ íŒŒì¼ URL
audio_url = "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"

# ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
audio_file = requests.get(audio_url).content
with open("temp_audio.flac", "wb") as f:
    f.write(audio_file)

# librosaë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë¡œë“œ
audio, sample_rate = librosa.load("temp_audio.flac", sr=16000)

# ì˜¬ë°”ë¥¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
inputs = feature_extractor(audio, sampling_rate=sample_rate, return_tensors="pt", padding=True)
print(f'inputs: {inputs}')
```

```python
# ê²°ê³¼
inputs: {'input_values': tensor([[ 0.0717,  0.0463, -0.0508,  ..., -0.0046,  0.0326, -0.1615]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}
```

## AutoModel

AutoModelFor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ì£¼ì–´ì§„ ì‘ì—…ì— ëŒ€í•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

`Pytorch` ì˜ˆì‹œ

```python
from transformers import AutoModelForSequenceClassification

# ì‹œí€€ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

##########################################
### ì˜ˆì œì— ì´ì–´ì„œ ì½”ë“œ ê³„ì† ì‘ì„±
##########################################
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

# ì…ë ¥ í…ìŠ¤íŠ¸
text = "This restaurant is awesome"

# í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
inputs = tokenizer(text, return_tensors="pt")
print(f'inputs: {inputs}')

# ëª¨ë¸ì— ì…ë ¥
outputs = model(**inputs)
print(f'outputs: {outputs}')
print(f'outputs logits: {outputs.logits[0]}')
```

```python
# ê²°ê³¼
inputs: {'input_ids': tensor([[  101,  2023,  4825,  2003, 12476,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}
# - input_ids: í† í°ì˜ ìˆ˜ì¹˜ì  í‘œí˜„ì…ë‹ˆë‹¤.
# - attention_mask: ì–´ë–¤ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.0002, -0.0701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
outputs logits: tensor([-0.0002, -0.0701], grad_fn=<SelectBackward0>)
```

- distilbert/distilbert-base-uncased
  - DistilBERTëŠ” BERT(Bidirectional Encoder Representations from Transformers)ì˜ ê²½ëŸ‰í™” ë²„ì „ì…ë‹ˆë‹¤. ì›ë˜ BERT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê±°ì˜ ìœ ì§€í•˜ë©´ì„œë„ í¬ê¸°ì™€ ì†ë„ ë©´ì—ì„œ ê°œì„ ëœ ëª¨ë¸ì…ë‹ˆë‹¤.
  - "base": ê¸°ë³¸ í¬ê¸°ì˜ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ëŒ€í˜• ëª¨ë¸ë„ ìˆìŠµë‹ˆë‹¤).
  - "uncased": í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

## ì¶”ê°€ ì˜ˆì‹œ - Quick Tour

> AutoTokenizer

```python
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(f'enconding: {encoding}')

##########################################
### ì…ë ¥ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥, padding, truncationë¡œ ê· ì¼í•œ ê¸¸ì´ì˜ ë°°ì¹˜ë¥¼ ë°˜í™˜
##########################################
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)

print(f'pt_batch: {pt_batch}')
```

```python
# ê²°ê³¼
enconding: {
'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
pt_batch: {
'input_ids': tensor([
                      [  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100, 58263, 13299,   119,   102],
                      [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,    102,     0,     0,     0]
                    ]), 
'token_type_ids': tensor([
                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
                    ]), 
'attention_mask': tensor([
                      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]
                     ])
}
```

- nlptown/bert-base-multilingual-uncased-sentiment
  - nlptown: ì´ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ê³µê°œí•œ ì¡°ì§ ë˜ëŠ” ì‚¬ìš©ìì˜ ì´ë¦„
  - bert: ì´ ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬ì¡°ê°€ BERT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©
  - base: BERTì˜ ê¸°ë³¸ í¬ê¸° ë²„ì „ -'small'ë³´ë‹¤ í¬ê³  'large'ë³´ë‹¤ ì‘ì€ ëª¨ë¸
  - multilingual: ë‹¤êµ­ì–´ ì§€ì›
  - uncased: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ
  - sentiment: ê°ì„± ë¶„ì„ - ê¸ì •/ë¶€ì •

> AutoModel

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_tokenizer = AutoTokenizer.from_pretrained(model_name)

pt_batch = pt_tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)

pt_outputs = pt_model(**pt_batch)

from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(f'pt_predictions: {pt_predictions}')
```

```python
# ê²°ê³¼
pt_predictions: 
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
# 1ì (ë§¤ìš° ë¶€ì •), 2ì (ë¶€ì •), 3ì (ì¤‘ë¦½), 4ì (ê¸ì •), 5ì (ë§¤ìš° ê¸ì •)ìœ¼ë¡œ ë ˆì´ë¸”ë§ëœ ê°ì„± ì ìˆ˜
```

---

í•´ì‹œíƒœê·¸: #Hugging-Face #Transformers #AutoClass #AutoTokenizer #AutoImageProcessor #AutoBackbone #AutoFeatureExtractor #AutoModel
