---
layout: single
title: "Hugging Face - 01. Transformers - Pipelines" 
categories: Hugging-Face
tag: [Hugging-Face, Transformers, Pipelines]
toc: true
toc_sticky: true
toc_label: "ëª©ì°¨"
---

## Transformer

ê³µì‹ë¬¸ì„œ: <https://huggingface.co/docs/transformers/index>

ìœ„ì˜ ê³µì‹ ë¬¸ì„œë¥¼ ì—°ìŠµ í•œ ê³³: <https://github.com/just-record/huggingface_practice>

ğŸ¤— TransformersëŠ” ìµœì‹ ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆëŠ” APIì™€ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install transformers, datasets, evaluate, accelerate
pip install torch       # PyTorch
pip install tensorflow  # TensorFlow
```

## Pipeline

<https://huggingface.co/docs/transformers/v4.44.0/en/main_classes/pipelines#transformers.pipeline>

`pipeline()`ì€ Hubì˜ ëª¨ë“  ëª¨ë¸ì„ ì–¸ì–´, ì»´í“¨í„° ë¹„ì „, ìŒì„±, ê·¸ë¦¬ê³  ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì— ëŒ€í•œ ì¶”ë¡ ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

## Task ì§€ì •

pipeline()ë¥¼ ìƒì„± í•  ë•Œ ì›í•˜ëŠ” ì‘ì—…ì„ ì§€ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•´ë‹¹ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# transformersì˜ pipelineë¥¼ import
from transformers import pipeline

# taskë¥¼ ì§€ì •í•˜ì—¬ pipelineì„ ìƒì„± - taskëŠ” text-classification
pipe = pipeline("text-classification")
# ìƒì„±ëœ pipelineì˜ ê°ì²´ì¸ pipeë¥¼ ì‚¬ìš©í•˜ì—¬ textë¥¼ ì…ë ¥í•˜ì—¬ ëª¨ë¸ì„ ì‹¤í–‰
outs = pipe("This restaurant is awesome")
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'POSITIVE', 'score': 0.9998743534088135}]
```

- taskë¥¼ ì§€ì •í•˜ì—¬ pipelineì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ ë•Œ ëª¨ë¸ì€ taskì˜ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `pipe.model`ì„ í†µí•´ ëª¨ë¸ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ëª¨ë¸ ì§€ì •

ëª¨ë¸ì„ ì§ì ‘ ì§€ì •í•˜ì—¬ pipelineì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import pipeline

pipe = pipeline(model="FacebookAI/roberta-large-mnli")
outs = pipe("This restaurant is awesome")
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'NEUTRAL', 'score': 0.7313143014907837}]
# NEUTRAL(ì¤‘ë¦½), CONTRADICTION(ëª¨ìˆœ), ENTAILMENT(ìˆ˜ë°˜) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜ë¨ 
```

- ëª¨ë¸ì„ ì§ì ‘ ì§€ì •í•˜ì—¬ pipelineì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì§€ì • ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- <https://huggingface.co/FacebookAI/roberta-large-mnli>ì—ì„œ ìœ„ì˜ ëª¨ë¸ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - NEUTRAL(ì¤‘ë¦½), CONTRADICTION(ëª¨ìˆœ), ENTAILMENT(ìˆ˜ë°˜) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.
  - 2ê°œì˜ ë¬¸ì¥ì´ ì…ë ¥ë˜ê³  ë‘ ë¬¸ì¥ì˜ ê´€ê³„ê°€ ì¤‘ë¦½ì¸ì§€, ëª¨ìˆœì¸ì§€, ìˆ˜ë°˜ë˜ëŠ”ì§€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.

### ìœ„ ëª¨ë¸ì— 2ê°œ ë¬¸ì¥ì„ ì…ë ¥

```python
from transformers import pipeline

pipe = pipeline(model="FacebookAI/roberta-large-mnli")

outs = pipe("I like you. I love you")
print(outs)
outs = pipe("I like you. I hate you")
print(outs)
outs = pipe("I like you. I don't hate you")
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'NEUTRAL', 'score': 0.7160009145736694}]
[{'label': 'CONTRADICTION', 'score': 0.9992231130599976}]
[{'label': 'ENTAILMENT', 'score': 0.7354484796524048}]
```

- 2ê°œì˜ ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ ê´€ê³„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
- `I like you. I love you`ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.
- `I like you. I hate you`ëŠ” ëª¨ìˆœìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.
- `I like you. I don't hate you`ëŠ” ìˆ˜ë°˜ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.

### ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©

ê³µì‹ ë¬¸ì„œì˜ ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ pipelineì„ ìƒì„±í•©ë‹ˆë‹¤.

- <https://huggingface.co/models?pipeline_tag=text-classification&sort=trending>: modelì—ì„œ 'text-classification'ìœ¼ë¡œ í•„í„°ë§ í•˜ì—¬ ëª©ë¡ì„ í™•ì¸í•©ë‹ˆë‹¤.
- `BAAI/bge-reranker-v2-m3` => <https://huggingface.co/BAAI/bge-reranker-v2-m3>: ì„ì˜ ì„ íƒ
- ìš°ì¸¡ ìƒë‹¨ 'Use in transformers' ë²„íŠ¼ í´ë¦­ -> 'Transformers' -> 'pipeline'ì´ ìˆëŠ” ì½”ë“œ ë³µì‚¬

```python
from transformers import pipeline

pipe = pipeline("text-classification", model="BAAI/bge-reranker-v2-m3")
outs = pipe("This restaurant is awesome")
print(outs)

######################################
### 2ê°œì˜ ë¬¸ì¥: ë¬¸ì¥ê°„ì˜ ê´€ë ¨ì„±ì„ ë¶„ë¥˜
######################################
print('-'*30)
outs = pipe([{"text": "'what is panda?'", "text_pair": "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."}])
print(outs)
outs = pipe([{"text": "'what is panda?'", "text_pair": "hi"}])
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'LABEL_0', 'score': 0.1711844801902771}]
# LABEL_0(ê´€ë ¨ì„±ì´ ë‚®ìŒ), LABEL_1(ê´€ë ¨ì„±ì´ ë†’ìŒ) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜ë¨
------------------------------
[{'label': 'LABEL_0', 'score': 0.9772558808326721}]
[{'label': 'LABEL_0', 'score': 0.00045623243204317987}]
```

### ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© - í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„

- <https://huggingface.co/models?pipeline_tag=text-classification&sort=trending>
- `mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis` => <https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis>
- ìš°ì¸¡ ìƒë‹¨ 'Use in transformers' ë²„íŠ¼ í´ë¦­ -> 'Transformers' -> 'pipeline'ì´ ìˆëŠ” ì½”ë“œ ë³µì‚¬

```python
from transformers import pipeline

pipe = pipeline("text-classification", model="mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis")
outs = pipe("This restaurant is awesome")
print(outs)
# positive, neutral, negative ì¤‘ í•˜ë‚˜ë¡œ

######################################
### ìœ„ ë¬¸ì¥ì˜ ê²°ê³¼ê°€ ì˜ˆìƒ ë°– -> ê¸ˆìœµ ë°ì´í„°ë¥¼ í•™ìŠµ í•œ ëª¨ë¸ì´ë¼ì„œ ê·¸ëŸ° ê²ƒìœ¼ë¡œ ì¶”ì •
# ëª¨ë¸ ì¹´ë“œì˜ ì„¤ëª…: 
#   This model is a fine-tuned version of distilroberta-base on the financial_phrasebank dataset. It achieves the following results on the evaluation set
#   => ì´ ëª¨ë¸ì€ financial_phrasebank ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •ëœ distilroberta-baseì˜ ë²„ì „ì…ë‹ˆë‹¤. í‰ê°€ ì„¸íŠ¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤:
######################################
print('-'*30)
outs = pipe("Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .")
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'neutral', 'score': 0.9986506104469299}]
------------------------------
[{'label': 'negative', 'score': 0.9987391829490662}]
```

## 2ê°œ ì´ìƒì˜ ë¬¸ì¥ ì…ë ¥

2ê°œ ì´ìƒì˜ ë¬¸ì¥ì„ list í˜•íƒœë¡œ ì…ë ¥í•˜ì—¬ ìš”ì²­ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import pipeline

pipe = pipeline("text-classification")
outs = pipe(["This restaurant is awesome", "This restaurant is awful"])
print(outs)
```

```python
# ê²°ê³¼
[{'label': 'POSITIVE', 'score': 0.9998743534088135}, {'label': 'NEGATIVE', 'score': 0.9998743534088135}]
```

- 2ê°œì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

## Dataset ì‚¬ìš©

Datasetì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Dataset - Iterator(yield ì‚¬ìš©)

yieldë¥¼ ì‚¬ìš©í•œ Iteratorë¥¼ í˜•íƒœì˜ Datasetì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import pipeline

# Iterator(yield ì‚¬ìš©)ë¥¼ ì‚¬ìš©í•œ Dataset ì‚¬ìš©
def data():
    for i in range(10):
        yield f"My example {i}"


pipe = pipeline(model="openai-community/gpt2")
generated_characters = 0

for out in pipe(data()):
    print(out[0]["generated_text"])
    generated_characters += len(out[0]["generated_text"])

print(f"Generated {generated_characters} characters in total.")    
```

```python
# ê²°ê³¼
...
My example 8-bit console runs on Windows (x86_64). I also have 2-bit 64 bit OS's I use on my Mac (x86_64)!
My example 9th grade teacher works in a church. He's a big fan of Harry Potter, and he knows he's right. He's not going to let someone else decide who he feels like to make his life harder. He knows one girl
Generated 2013 characters in total.
```

### Dataset -  Hugging Faceì˜ Datasets

Hugging Faceì˜ Datasetsì„ ì‚¬ìš©í•˜ì—¬ Datasetì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2")
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

```python
# ê²°ê³¼
{'text': "EYB  ZB COE C BEZCYCZ HO MOWWB EM BWOB ZMEG  B COEB BE BEC B U OB BE BCB BEWUBB BXYWBESWYCB SBBB SSEZ C Z WH UB F IGVB SB Z<unk> XOES CZ BBXOXFBB  OBY W B VM OFOWUONFWB ZCX B M WZ Q S C Q BC CQBF FOMB BOT ZWYBZ WB  B CM B C B WZCWWW BHU EOYTO YWB BZ SHZBGEM Q OO T B BM XZ QW C OFBZMSEHB BE ZZBX M Q XB<unk> CEVWZ FOHSB W B O Z ZW S ZB O VM <s> D EUCKH XNC D Q BG B O BW U  U  MBE CBYE  WB HFQUBQBUWZ B MW BMPY F ZBU  EB B WBOF S XFOBB ZB X B MOT W B CEO WBM   BBXBBEOBECB B UM C BP FMBWB BZ WFCED Z B B FXB Z OZ OBBZ NVD UBZC W B WYCWY X CE CW B WB MWU BWN B DECF GEF'C WZS CS BYWB<s>FZ'Z<s>ZGBU ECFEY BF ZOZ O UWBSSZBBBBW   O O DBB BZWFUW ZWOZYCGOYCOT WC O CZ BD BBBBBBX X W T B BC BZC FWYBFO FBCE X Z PEZ CE B WEDBMBO BN B BY Y  W B BMCB XOXQ  BSZES Z M CF S FB BBXBB B C CSZ EF SEQF S BEC BNO BN  SU EH  WRFBS WB  W B OEZ WS X B F B X ZBBE BBEHB B BU BECBSXHB BSQWFW BSZXH BWSEG W VQETZMCZ UCXW Z DBE<s> O SXZX MB W RX YYOBSUBWOCFYEF O B O B C Z UBEZBE BTB C   CBFCB V W B BF W ZBBESBBECUT OH YCGZE W BYZBYGBEW BSBE B  B CBEBEBU BU OQB OSB CB Z Z Z HZGTXZ USZFB FC ZO C"}
```

- ê²°ê³¼ê°€ textë¡œ ë‚˜ì˜¤ì§€ ì•ŠìŒ
- hf-internal-testing/librispeech_asr_dummyì´ ëª¨ë¸ ëª©ë¡ì—ì„œ ê²€ìƒ‰ ë˜ì§€ ì•ŠìŒ
- hf-internal-testing ì—ì„œ testìš© ëª¨ë¸ ì¸ ë“¯

### Dataset - Hugging Faceì˜ Datasets - ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©

```python
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="openai/whisper-large-v2")
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

```python
# ê²°ê³¼
{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'}
{'text': " Nor is Mr. Quilter's manner less interesting than his matter."}
{'text': ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.'}
{'text': " He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it but little of Rocky Ithaca."}
{'text': " Linnell's pictures are a sort of up-guards-and-addams paintings, and Mason's exquisite idylls are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. And Mr. John Collier gives his sitter a cheerful slap on the back, before he says, like a shampoo-er in a Turkish bath,"}
{'text': ' It is obviously unnecessary for us to point out how luminous these criticisms are, how delicate in expression.'}
{'text': ' On the general principles of art, Mr. Quilter writes with equal lucidity.'}
{'text': ' Painting, he tells us, is of a different quality to mathematics, and Finnish in art is adding more factor.'}
{'text': ' As for etchings, they are of two kinds, British and foreign.'}
{'text': ' He laments most bitterly the divorce that has been made between decorative art and what we usually call pictures, makes a customary appeal to the last judgment, and reminds us that in the great days of art Michelangelo was the furnishing upholsterer.'}
```

### Dataset - Hugging Faceì˜ Datasets - ë‹¤ë¥¸ Dataset ì‚¬ìš©

ê³µì‹ ë¬¸ì„œì˜ ì˜ˆì œê°€ ì•„ë‹Œ ë‹¤ë¥¸ Datasetì„ ì‚¬ìš© í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

- `PolyAI/minds14`
- <https://huggingface.co/docs/transformers/quicktour#pipeline>

```python
import torch
from transformers import pipeline
from datasets import load_dataset, Audio

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", trust_remote_code=True)
dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])
```

```python
# ê²°ê³¼
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

## Parameter

'pipeline()'ì€ ë§ì€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì¼ë¶€ëŠ” íŠ¹ì • ì‘ì—…ì— ê´€ë ¨ë˜ê³ , ì¼ë¶€ëŠ” ëª¨ë“  íŒŒì´í”„ë¼ì¸ì— ê³µí†µì ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì›í•˜ëŠ” ê³³ ì–´ë””ì—ì„œë‚˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Device, batch_size

- `device` - '0': ì²«ë²ˆì§¸ GPU, '1': ë‘ë²ˆì§¸ GPU ...   '-1': CPU
- `device_map` - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ë¡œë“œí•˜ê³  ì €ì¥í• ì§€ ìë™ìœ¼ë¡œ ê²°ì •
- `batch_size` - ë°°ì¹˜ í¬ê¸°
  - ê¸°ë³¸ì ìœ¼ë¡œ pipelineì€ ë°°ì¹˜ ì¶”ë¡ ì„ í•˜ì§€ ì•Šì§€ë§Œ ì‚¬ìš© í•  ê²½ìš°ì˜ ì˜ˆì‹œ

```python
from transformers import pipeline

##########################################
### 1. device=0 ì‚¬ìš©
# device=0: ì²«ë²ˆì§¸ GPU ì‚¬ìš© - GPUê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
# device=-1: CPU ì‚¬ìš©
##########################################
pipe = pipeline(model="openai/whisper-large-v2", device=0)
# pipe = pipeline(model="openai/whisper-large-v2", device=-1)
response = pipe("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
print(response)

##########################################
### 2. device_map="auto" ì‚¬ìš© - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ë¡œë“œí•˜ê³  ì €ì¥í• ì§€ ìë™ìœ¼ë¡œ ê²°ì •
# device_map="auto"ë¥¼ ì‚¬ìš© í•˜ë©´ 'device=device'ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì•„ë„ ë¨
##########################################
print('-'*30)
# pipe = pipeline(model="openai/whisper-large-v2", device_map="auto", low_cpu_mem_usage=True)
pipe = pipeline(model="openai/whisper-large-v2", device_map="auto")
response = pipe("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
print(response)

##########################################
### 3. batch_size ì‚¬ìš© - í•œ ë²ˆì— ì²˜ë¦¬í•  ìŒì„± íŒŒì¼ì˜ ê°œìˆ˜
# batch_size=2: ì˜¤ë¥˜ ë°œìƒ - IndexError: tuple index out of range => ì›ì¸ì„ ì°¾ì•„ì•¼ í•¨
# batch_size=1: ì •ìƒ ì²˜ë¦¬
##########################################
print('-'*30)
# pipe = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
pipe = pipeline(model="openai/whisper-large-v2", device=0, batch_size=1)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = pipe(audio_filenames)
print(texts)
```

```python
# ê²°ê³¼
[{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}]
------------------------------
[{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}]
------------------------------
[{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered, flour-fattened sauce.'}, {'text': ' Stuff it into you, his belly counselled him.'}, {'text': ' After early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels.'}, {'text': ' Y en las ramas medio sumergidas revoloteaban algunos pÃ¡jaros de quimÃ©rico y legendario plumaje.'}]
```

### Parameter - Taskì— ë”°ë¥¸ Parameter

```python
from transformers import pipeline

##########################################
### 'return_timestamps' - ë°œìŒëœ ì‹œê°„
##########################################
# pipe = pipeline(model="openai/whisper-large-v2", device=0, return_timestamps=True)
pipe = pipeline(model="openai/whisper-large-v2", device=-1, return_timestamps=True)
response = pipe("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
print(response)
# {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}


##########################################
### 'chunk_length_s' - ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì‘ì€ ì²­í¬(chunk)ë¡œ ë‚˜ëˆ„ë©° ê° ì²­í¬ì˜ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ì§€ì •
##########################################
print('-'*30)
# pipe = pipeline(model="openai/whisper-large-v2", device=0, chunk_length_s=30)
pipe = pipeline(model="openai/whisper-large-v2", device=-1, chunk_length_s=30)
response = pipe("https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav")
print(response)
# {'text': " So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know. You get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light and I'd bump it up"}
```

```python
# ê²°ê³¼
[{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}]
------------------------------
[{'text': " So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know. You get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light and I'd bump it up"}]
```

ì—ëŸ¬ ë°œìƒ

```bash
# ValueError: ffmpeg was not found but is required to load audio files from filename
# ìœ„ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ffmpegë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
sudo apt update
sudo apt install ffmpeg
```

## ì§€ì› ë˜ëŠ” Tasks

ì†ŒìŠ¤ ì½”ë“œë¥¼ ì°¸ì¡°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ taskë“¤ì„ ì§€ì›í•˜ëŠ” ê±¸ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<https://github.com/huggingface/transformers/blob/v4.43.3/src/transformers/pipelines/__init__.py#L552>

- `"audio-classification"`: will return a [`AudioClassificationPipeline`].
- `"automatic-speech-recognition"`: will return a [`AutomaticSpeechRecognitionPipeline`].
- `"depth-estimation"`: will return a [`DepthEstimationPipeline`].
- `"document-question-answering"`: will return a [`DocumentQuestionAnsweringPipeline`].
- `"feature-extraction"`: will return a [`FeatureExtractionPipeline`].
- `"fill-mask"`: will return a [`FillMaskPipeline`]:.
- `"image-classification"`: will return a [`ImageClassificationPipeline`].
- `"image-feature-extraction"`: will return an [`ImageFeatureExtractionPipeline`].
- `"image-segmentation"`: will return a [`ImageSegmentationPipeline`].
- `"image-to-image"`: will return a [`ImageToImagePipeline`].
- `"image-to-text"`: will return a [`ImageToTextPipeline`].
- `"mask-generation"`: will return a [`MaskGenerationPipeline`].
- `"object-detection"`: will return a [`ObjectDetectionPipeline`].
- `"question-answering"`: will return a [`QuestionAnsweringPipeline`].
- `"summarization"`: will return a [`SummarizationPipeline`].
- `"table-question-answering"`: will return a [`TableQuestionAnsweringPipeline`].
- `"text2text-generation"`: will return a [`Text2TextGenerationPipeline`].
- `"text-classification"` (alias `"sentiment-analysis"` available): will return a [`TextClassificationPipeline`].
- `"text-generation"`: will return a [`TextGenerationPipeline`]:.
- `"text-to-audio"` (alias `"text-to-speech"` available): will return a [`TextToAudioPipeline`]:.
- `"token-classification"` (alias `"ner"` available): will return a [`TokenClassificationPipeline`].
- `"translation"`: will return a [`TranslationPipeline`].
- `"translation_xx_to_yy"`: will return a [`TranslationPipeline`].
- `"video-classification"`: will return a [`VideoClassificationPipeline`].
- `"visual-question-answering"`: will return a [`VisualQuestionAnsweringPipeline`].
- `"zero-shot-classification"`: will return a [`ZeroShotClassificationPipeline`].
- `"zero-shot-image-classification"`: will return a [`ZeroShotImageClassificationPipeline`].
- `"zero-shot-audio-classification"`: will return a [`ZeroShotAudioClassificationPipeline`].
- `"zero-shot-object-detection"`: will return a [`ZeroShotObjectDetectionPipeline`].

## Vision, Audio, Multimodal

Vision, Audio, Multimodal í˜•íƒœì˜ Pipelineë„ ì§€ì› í•˜ë©´ ì‚¬ìš© ë°©ë²•ì€ ê±°ì˜ ë™ì¼í•©ë‹ˆë‹¤.

### Vision Pipeline

```python
from transformers import pipeline

vision_classifier = pipeline(model="google/vit-base-patch16-224", device=0)
preds = vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
print(preds)
```

```python
# ê²°ê³¼
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

### Audio Pipeline

```python
from transformers import pipeline

transcriber = pipeline(task="automatic-speech-recognition")

response = transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
print(response)
```

```python
# ê²°ê³¼
[{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}]
```

### Multimodal Pipeline

```bash
# ì„¤ì¹˜ í•„ìš”
sudo apt install -y tesseract-ocr
pip install pytesseract
```

```python
from transformers import pipeline

vqa = pipeline(model="impira/layoutlm-document-qa", device=0)
output = vqa(
    image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
    question="What is the invoice number?",
)
output[0]["score"] = round(output[0]["score"], 3)
print(output[0]["score"])
```

```python
# ê²°ê³¼
[{'answer': 'INV-123', 'score': 0.999}]
```

## Large Model - with accelerate

Hugging Faceì˜ accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
print(output)
```

```python
# ê²°ê³¼
[{'generated_text': 'This is a cool example!  How did you get a pic of the front of it?'}]
```

## Gradioë¡œ ì›¹ì„œë²„ êµ¬ë™

```bash
pip install gradio
```

```python
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```

```plantext
http://127.0.0.1:7860 - ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
```

![ì‹¤í–‰ì˜ˆì‹œ - ì¶œì²˜: ê³µì‹ ë¬¸ì„œ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/panda-classification.png)

## Text Generation

ì§€ê¸ˆ ê¹Œì§€ í•™ìŠµ í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœê·¼ ëª¨ë¸ì˜ Text Generationì„ ì‹¤í–‰í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

- ëª¨ë¸: `meta-llama/Meta-Llama-3.1-8B-Instruct`
- ëª¨ë¸ ì‚¬ìš© ì‹œ ì¸ì¦ í•„ìš”
- Hugging Faceì˜ Access Token í•„ìš”

### ëª¨ë¸

<https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct>

### ëª¨ë¸ ì‚¬ìš© ì‹œ ì¸ì¦ í•„ìš”

- ì…ë ¥ í•­ëª©: 'First Name', 'Last Name', 'Data of birth', 'Country', 'Affitiation', 'Job title'
- 'ë¼ì´ì„ ìŠ¤ ë™ì˜', 'ê°œì¸ì •ë³´ ì²˜ë¦¬ ë™ì˜'
- ëª¨ë¸ ì†Œìœ ìì˜ ìŠ¹ì¸ ëŒ€ê¸° -> ìŠ¹ì¸ -> ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

### Hugging Faceì˜ Access Token í•„ìš”

- ë¡œê·¸ì¸ -> 'Profile'(ìš°ì¸¡ ìƒë‹¨) -> 'Settings' -> 'Access Tokens' -> '+Create new token'
- Token type: Write, Token name: xxx -> 'Create token' -> Copy

### ì½”ë“œ

```python
from dotenv import load_dotenv
load_dotenv()

from huggingface_hub import login
import os

result = login(token=os.environ['HUGGING_FACE_HUB_TOKEN'])
print(result)


from transformers import pipeline

messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe = pipeline("text-generation", model="meta-llama/Meta-Llama-3.1-8B-Instruct", device=0)
outs = pipe(messages)
print(outs)
```

```python
# ê²°ê³¼
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 119.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
```

- CUDA out of memory ì—ëŸ¬ ë°œìƒ
- ìµœê·¼ ëª¨ë¸ì€ ì‹¤íŒ¨ í–ˆì§€ë§Œ ì‹œê°„ì„ ë‘ê³  ë‹¤ë¥¸ ëª¨ë¸ ë„ì „ ë˜ëŠ” ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²° ì‹œë„

---

í•´ì‹œíƒœê·¸: #Hugging-Face #Transformers #Pipelines #Task #Model #Dataset #Parameter #Device #batch_size #Vision #Audio #Multimodal #Text-Generation #Gradio
